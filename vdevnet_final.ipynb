{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3853887f",
   "metadata": {},
   "source": [
    "# Variational Deviation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe446ab",
   "metadata": {},
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62cac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csc_matrix\n",
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tensorflow.keras import backend as K, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328bf6a",
   "metadata": {},
   "source": [
    "## data loader functoins + sampling using Autoencoder outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem = Memory(\"./dataset/svm_data\", verbose=0)\n",
    "\n",
    "@mem.cache\n",
    "def get_data_from_svmlight_file(path):\n",
    "    X, y = load_svmlight_file(path)\n",
    "    return X.toarray(), y\n",
    "\n",
    "def dataLoading(path):\n",
    "    df = pd.read_csv(path)\n",
    "    labels = df['class'].values\n",
    "    x = df.drop(['class'], axis=1).values\n",
    "    return x, labels\n",
    "\n",
    "# sampling function used in VDevNet, based on the reparameterization trick.\n",
    "# this function takes the mean and log varianec from the encoder \n",
    "# to generate a latent vector that follows a gaussian\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f28788",
   "metadata": {},
   "source": [
    "## special callback for aucpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to compute aupr on validation data each epoch\n",
    "# it predicts on x_val and stores score in logs\n",
    "class AUC_Callback(Callback):\n",
    "    def __init__(self, x_val, y_val):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "        if y_pred.shape[-1] == 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        logs = logs or {}\n",
    "        logs['val_aupr'] = average_precision_score(self.y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b2eeb",
   "metadata": {},
   "source": [
    "## AUTOENCODER + DECODER ARCHITECTUER FOR VARIANCE AND MEAN CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95eab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a variational autoencoder for anomaly detection\n",
    "# it learns to compress input into a small latent space and then reconstruct it\n",
    "# we use mean and log variance to sample latent vector z\n",
    "# loss combines reconstruction error and kl divergence to shape latent distribution\n",
    "def build_vae(input_dim, latent_dim=2):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "    d = Dense(64, activation='relu')(z)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dense(128, activation='relu')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    outputs = Dense(input_dim, activation='linear')(d)\n",
    "\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "    recon_loss = tf.reduce_mean(tf.square(inputs - outputs)) * input_dim\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    vae.add_loss(recon_loss + kl_loss)\n",
    "    vae.compile(optimizer=AdamW(learning_rate=1e-3))\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "    return vae, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74542a8",
   "metadata": {},
   "source": [
    "## architecture, loss + helper functions(noise injedction and batch generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deviation-based loss that pushes normal samples close to the reference distribution\n",
    "# and forces anomalies to deviate by at least the specified margin\n",
    "def create_vdev_loss(mu_R, sigma_R, margin=5.0):\n",
    "    def deviation_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, 'float32')\n",
    "        dev = (y_pred - mu_R) / (sigma_R + K.epsilon())\n",
    "        inlier_loss = K.abs(dev)\n",
    "        outlier_loss = K.abs(K.maximum(margin - dev, 0.0))\n",
    "        return K.mean((1 - y_true) * inlier_loss + y_true * outlier_loss)\n",
    "    return deviation_loss\n",
    "\n",
    "\n",
    "# define a deep deviation network with three hidden layers for complex data\n",
    "def dev_network_d(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(250, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# this generator makes batches with equal mix of normal and outlier samples\n",
    "# it picks half batch from outliers with replacement and half from inliers without replacement\n",
    "# then shuffles and yields the data and labels indicating which are outliers\n",
    "def batch_generator_sup(x, out_idx, in_idx, batch_size, rng):\n",
    "    n_out = max(1, batch_size // 2)\n",
    "    while True:\n",
    "        out_samples = rng.choice(out_idx, n_out, replace=True)\n",
    "        in_samples = rng.choice(in_idx, batch_size - n_out, replace=False)\n",
    "        idx = np.concatenate([in_samples, out_samples])\n",
    "        rng.shuffle(idx)\n",
    "        labels = np.isin(idx, out_idx).astype(np.float32)\n",
    "        yield x[idx], labels\n",
    "\n",
    "# create synthetic samples by mixing features 5% swapped\n",
    "def inject_noise(seed, n_out, random_seed):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    n_sample, dim = seed.shape\n",
    "    swap_ratio = 0.05\n",
    "    n_swap = int(dim * swap_ratio)\n",
    "    i1 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    i2 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    idxs = rng.choice(dim, size=(n_out, n_swap), replace=True)\n",
    "    noise = seed[i1].copy()\n",
    "    rows = np.arange(n_out)[:, None]\n",
    "    noise[rows, idxs] = seed[i2[:, None], idxs]\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a3e5a",
   "metadata": {},
   "source": [
    "## Vdevnet final run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadc233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[annthyroid_21feat_normalised] Training VAE...\n",
      "146/146 [==============================] - 0s 687us/step\n",
      "34/34 [==============================] - 0s 1ms/step\n",
      "[annthyroid_21feat_normalised] ROC AUC=0.9651, AUPR=0.9324\n",
      "[bank-additional-full_normalised] Training VAE...\n",
      "800/800 [==============================] - 1s 842us/step\n",
      "194/194 [==============================] - 0s 2ms/step\n",
      "[bank-additional-full_normalised] ROC AUC=0.9266, AUPR=0.5851\n",
      "[celeba_baldvsnonbald_normalised] Training VAE...\n",
      "4333/4333 [==============================] - 4s 889us/step\n",
      "950/950 [==============================] - 2s 2ms/step\n",
      "[celeba_baldvsnonbald_normalised] ROC AUC=0.9663, AUPR=0.3519\n",
      "[census-income-full-mixed-binarized] Training VAE...\n",
      "6141/6141 [==============================] - 7s 1ms/step\n",
      "1403/1403 [==============================] - 7s 5ms/step\n",
      "[census-income-full-mixed-binarized] ROC AUC=0.8986, AUPR=0.5614\n",
      "[creditcardfraud_normalised] Training VAE...\n",
      "6220/6220 [==============================] - 5s 801us/step\n",
      "1336/1336 [==============================] - 3s 2ms/step\n",
      "[creditcardfraud_normalised] ROC AUC=0.9103, AUPR=0.6862\n",
      "[KDD2014_donors_10feat_nomissing_normalised] Training VAE...\n",
      "12745/12745 [==============================] - 13s 997us/step\n",
      "2904/2904 [==============================] - 7s 2ms/step\n",
      "[KDD2014_donors_10feat_nomissing_normalised] ROC AUC=1.0000, AUPR=1.0000\n",
      "[UNSW_NB15_traintest_backdoor] Training VAE...\n",
      "2035/2035 [==============================] - 2s 1ms/step\n",
      "447/447 [==============================] - 1s 3ms/step\n",
      "[UNSW_NB15_traintest_backdoor] ROC AUC=0.9862, AUPR=0.9392\n",
      "All results saved to all_dataset_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_vdevnet(config):\n",
    "    results = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for fname in os.listdir(config.input_path):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        name = fname.rsplit('.',1)[0]\n",
    "        X, y = dataLoading(os.path.join(config.input_path, fname))\n",
    "\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, stratify=y, random_state=config.random_seed)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=config.random_seed)\n",
    "        \n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        out_idx = np.where(y_train==1)[0]\n",
    "        in_idx = np.where(y_train==0)[0]\n",
    "        \n",
    "        n_synth_outliers = int(len(out_idx) * 0.5)  # Add 50% more synthetic outliers\n",
    "        synth_outliers = inject_noise(X_train[out_idx], n_synth_outliers, config.random_seed)\n",
    "        \n",
    "        X_train_aug = np.vstack([X_train, synth_outliers])\n",
    "        y_train_aug = np.concatenate([y_train, np.ones(n_synth_outliers)])  # Label as outliers\n",
    "            \n",
    "        in_idx = np.where(y_train_aug==0)[0]\n",
    "        out_idx = np.where(y_train_aug==1)[0]\n",
    "\n",
    "        n_noise = int(len(in_idx) * config.cont_rate / (1 - config.cont_rate))\n",
    "        synth = inject_noise(X_train[in_idx], n_noise, config.random_seed)\n",
    "        \n",
    "        X_train_aug = np.vstack([X_train, synth])\n",
    "        y_train_aug = np.concatenate([y_train, np.ones(n_noise)])\n",
    "        \n",
    "        in_idx = np.where(y_train_aug==0)[0]\n",
    "        out_idx = np.where(y_train_aug==1)[0]\n",
    "\n",
    "        print(f\"[{name}] Training VAE...\")\n",
    "        vae, encoder = build_vae(input_dim=X_train.shape[1], latent_dim=config.latent_dim)\n",
    "        vae.fit(X_train[in_idx], epochs=config.epochs, batch_size=config.batch_size, verbose=0)\n",
    "        z_mean, z_log_var = encoder.predict(X_train[in_idx])\n",
    "        mu_R = np.mean(z_mean)\n",
    "        sigma_R = np.sqrt(np.mean(np.exp(z_log_var)))\n",
    "\n",
    "        model = dev_network_d(input_shape=(X_train_aug.shape[1],))\n",
    "        loss_fn = create_vdev_loss(mu_R, sigma_R, config.margin)\n",
    "        optimizer = AdamW(learning_rate=config.lr, weight_decay=config.weight_decay)\n",
    "        model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "        callbacks = [\n",
    "            AUC_Callback(X_val, y_val),\n",
    "            ModelCheckpoint(os.path.join(config.model_dir, f\"vdevnet_{name}.keras\")),\n",
    "            ReduceLROnPlateau(monitor='val_aupr', mode='max', factor=0.5, patience=5, min_lr=1e-6),\n",
    "            EarlyStopping(monitor='val_aupr', mode='max', patience=10, restore_best_weights=True)\n",
    "        ]\n",
    "\n",
    "        steps = max(1, len(in_idx)//config.batch_size)\n",
    "        model.fit(\n",
    "            batch_generator_sup(X_train_aug, out_idx, in_idx, config.batch_size, np.random),\n",
    "            steps_per_epoch=steps,\n",
    "            epochs=config.epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        y_score = model.predict(X_test)\n",
    "        if y_score.shape[-1]==1:\n",
    "            y_score = y_score.flatten()\n",
    "        roc = roc_auc_score(y_test, y_score)\n",
    "        aupr = average_precision_score(y_test, y_score)\n",
    "        print(f\"[{name}] ROC AUC={roc:.4f}, AUPR={aupr:.4f}\")\n",
    "        results.append({'dataset':name, 'roc':roc, 'aupr':aupr})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(config.output_csv, index=False)\n",
    "    print(\"All results saved to\", config.output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Config: pass\n",
    "    cfg = Config()\n",
    "    cfg.input_path = './dataset/'\n",
    "    cfg.model_dir = './model/'\n",
    "    cfg.output_csv = 'all_dataset_results.csv'\n",
    "    cfg.latent_dim = 4\n",
    "    cfg.known_outliers = 30\n",
    "    cfg.cont_rate = 0.02\n",
    "    cfg.batch_size = 512\n",
    "    cfg.epochs = 80\n",
    "    cfg.lr = 5e-3\n",
    "    cfg.weight_decay = 1e-5\n",
    "    cfg.margin = 6.0\n",
    "    cfg.random_seed = 42\n",
    "    os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "    run_vdevnet(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7738c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
