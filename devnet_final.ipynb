{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6af726",
   "metadata": {},
   "source": [
    "# DEVNET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60863e",
   "metadata": {},
   "source": [
    "## includes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from joblib import Memory\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9787d1",
   "metadata": {},
   "source": [
    "# helper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./dataset/svm_data\", verbose=0)\n",
    "\n",
    "def dataLoading(path):\n",
    "    df = pd.read_csv(path)\n",
    "    labels = df['class'].values\n",
    "    x = df.drop(['class'], axis=1).values\n",
    "    return x, labels\n",
    "\n",
    "@mem.cache\n",
    "def get_data_from_svmlight_file(path):\n",
    "    X, y = load_svmlight_file(path)\n",
    "    return X.toarray(), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade2a32",
   "metadata": {},
   "source": [
    "## callbacks for auc-pr maximization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track aupr on a validation set\n",
    "# designed for anomaly detection tasks aupr more informative\n",
    "# aupr stored in the logs after each epoch\n",
    "class AUC_Callback(Callback):\n",
    "    def __init__(self, x_val, y_val):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "        if y_pred.shape[-1] == 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        val_aupr = average_precision_score(self.y_val, y_pred)\n",
    "        if logs is not None:\n",
    "            logs['val_aupr'] = val_aupr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767ef3d",
   "metadata": {},
   "source": [
    "## deviation loss definition and network architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc464c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deviation-based loss that pushes normal samples close to the reference distribution\n",
    "# and forces anomalies to deviate by at least the specified margin\n",
    "def create_deviation_loss(margin=5.0, ref_size=5000):\n",
    "    ref = K.variable(np.random.normal(size=ref_size), dtype='float32')\n",
    "    \n",
    "    def deviation_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, 'float32')\n",
    "        dev = (y_pred - K.mean(ref)) / (K.std(ref) + K.epsilon())\n",
    "        inlier_loss = K.abs(dev)\n",
    "        outlier_loss = K.abs(K.maximum(margin - dev, 0.0))\n",
    "        return K.mean((1 - y_true) * inlier_loss + y_true * outlier_loss)\n",
    "    \n",
    "    return deviation_loss\n",
    "\n",
    "\n",
    "# define a deep deviation network with three hidden layers for complex data\n",
    "def dev_network_d(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(250, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# define a shallow deviation network with one hidden layer for simpler data\n",
    "def dev_network_s(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# define a linear deviation network without hidden layers for baseline comparison\n",
    "def dev_network_linear(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    out = Dense(1, activation='linear')(inp)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# assemble the deviation network of specified depth, compile with AdamW optimizer and deviation loss\n",
    "def deviation_network(input_shape, depth, lr, wd, margin):\n",
    "    if depth == 4:\n",
    "        model = dev_network_d(input_shape)\n",
    "    elif depth == 2:\n",
    "        model = dev_network_s(input_shape)\n",
    "    elif depth == 1:\n",
    "        model = dev_network_linear(input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported network depth\")\n",
    "    optimizer = AdamW(learning_rate=lr, weight_decay=wd)\n",
    "    loss_fn = create_deviation_loss(margin)\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f22bc",
   "metadata": {},
   "source": [
    "## data preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generator makes batches with equal mix of normal and outlier samples\n",
    "# it picks half batch from outliers with replacement and half from inliers without replacement\n",
    "# then shuffles and yields the data and labels indicating which are outliers\n",
    "def batch_generator_sup(x, out_idx, in_idx, batch_size, rng):\n",
    "    n_out_batch = max(1, batch_size // 2)\n",
    "    while True:\n",
    "        out_samples = rng.choice(out_idx, n_out_batch, replace=True)\n",
    "        in_samples = rng.choice(in_idx, batch_size - n_out_batch, replace=False)\n",
    "        batch_idx = np.concatenate([in_samples, out_samples])\n",
    "        rng.shuffle(batch_idx)\n",
    "        labels = np.isin(batch_idx, out_idx).astype(np.float32)\n",
    "        yield x[batch_idx], labels\n",
    "\n",
    "# create synthetic samples by mixing features 5% swapped\n",
    "def inject_noise(seed, n_out, random_seed):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    n_sample, dim = seed.shape\n",
    "    swap_ratio = 0.05\n",
    "    n_swap = int(dim * swap_ratio)\n",
    "\n",
    "    i1 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    i2 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    idxs = rng.choice(dim, size=(n_out, n_swap), replace=True)\n",
    "\n",
    "    noise = seed[i1].copy()\n",
    "    rows = np.arange(n_out)[:, None]\n",
    "    noise[rows, idxs] = seed[i2[:, None], idxs]\n",
    "\n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466a16e",
   "metadata": {},
   "source": [
    "## train + test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0acec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole training and testing for devnet on many files\n",
    "# first load data, split it fair, scale features, make extra outlier samples, then fit model and record scores\n",
    "# also use tools to stop early, change learning speed, and save best model by aupr check\n",
    "def run_devnet(config):\n",
    "    all_results = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for fname in os.listdir(config.input_path):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        name = fname.rsplit('.', 1)[0]\n",
    "        x, y = dataLoading(os.path.join(config.input_path, fname))\n",
    "\n",
    "        x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, stratify=y, random_state=config.random_seed)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=config.random_seed)\n",
    "\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        out_idx = np.where(y_train == 1)[0]\n",
    "        in_idx = np.where(y_train == 0)[0]\n",
    "        if len(out_idx) > config.known_outliers:\n",
    "            drop = np.random.choice(out_idx, len(out_idx) - config.known_outliers, replace=False)\n",
    "            keep = np.setdiff1d(np.arange(len(y_train)), drop)\n",
    "            x_train, y_train = x_train[keep], y_train[keep]\n",
    "            out_idx = np.where(y_train == 1)[0]\n",
    "            in_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "        n_noise = int(len(in_idx) * config.cont_rate / (1 - config.cont_rate))\n",
    "        synth = inject_noise(x_train[out_idx], n_noise, config.random_seed)\n",
    "        x_train = np.vstack([x_train, synth])\n",
    "        y_train = np.concatenate([y_train, np.zeros(n_noise)])\n",
    "        in_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "        model = deviation_network(\n",
    "            input_shape=(x_train.shape[1],),\n",
    "            depth=config.network_depth,\n",
    "            lr=config.lr,\n",
    "            wd=config.weight_decay,\n",
    "            margin=config.margin\n",
    "        )\n",
    "\n",
    "        ckpt = ModelCheckpoint(f\"./model/devnet_{name}.keras\", save_best_only=True, monitor='val_aupr', mode='max')\n",
    "        es_cb = EarlyStopping(monitor='val_aupr', mode='max', patience=10, restore_best_weights=True)\n",
    "        lr_cb = ReduceLROnPlateau(monitor='val_aupr', mode='max', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        auc_cb = AUC_Callback(x_val, y_val)\n",
    "\n",
    "        steps = max(1, len(in_idx) // config.batch_size)\n",
    "        model.fit(\n",
    "            batch_generator_sup(x_train, out_idx, in_idx, config.batch_size, np.random),\n",
    "            steps_per_epoch=steps,\n",
    "            epochs=config.epochs,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[auc_cb, ckpt, lr_cb, es_cb],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        y_score = model.predict(x_test)\n",
    "        roc = roc_auc_score(y_test, y_score)\n",
    "        aupr = average_precision_score(y_test, y_score)\n",
    "        print(f\"{name}: ROC AUC={roc:.4f}, AUPR={aupr:.4f}\")\n",
    "        all_results.append({'dataset': name, 'roc': roc, 'aupr': aupr})\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    output_csv_filename = \"all_dataset_results6\"\n",
    "    results_df.to_csv(output_csv_filename, index=False)\n",
    "    print(\"Summary results saved to\", output_csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b710256",
   "metadata": {},
   "source": [
    "## function call using configuration set as the following\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f46ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 937us/step\n",
      "annthyroid_21feat_normalised: ROC AUC=0.9618, AUPR=0.8574\n",
      "194/194 [==============================] - 0s 626us/step\n",
      "bank-additional-full_normalised: ROC AUC=0.7964, AUPR=0.4064\n",
      "950/950 [==============================] - 1s 693us/step\n",
      "celeba_baldvsnonbald_normalised: ROC AUC=0.8843, AUPR=0.1987\n",
      "1403/1403 [==============================] - 1s 783us/step\n",
      "census-income-full-mixed-binarized: ROC AUC=0.7042, AUPR=0.2434\n",
      "1336/1336 [==============================] - 1s 728us/step\n",
      "creditcardfraud_normalised: ROC AUC=0.9347, AUPR=0.6714\n",
      "2904/2904 [==============================] - 2s 846us/step\n",
      "KDD2014_donors_10feat_nomissing_normalised: ROC AUC=1.0000, AUPR=1.0000\n",
      "447/447 [==============================] - 0s 678us/step\n",
      "UNSW_NB15_traintest_backdoor: ROC AUC=0.9654, AUPR=0.9151\n",
      "Summary results saved to all_dataset_results6\n"
     ]
    }
   ],
   "source": [
    "# choose network size: 1=linear, 2=shallow, 4=deep deviation network\n",
    "# known_outliers: num outliers to bbe kept\n",
    "# contam = num outliers\n",
    "# lr =  learning rate\n",
    "# weight decay = L2 regularization\n",
    "# margin = how far outliers must deviate from normal\n",
    "if __name__ == \"__main__\":\n",
    "    class Config:\n",
    "        pass\n",
    "    # various configs like contamination rate, network depth to choose netork, batch sixze, margin, weight decay, learn rate, etc\n",
    "    cfg = Config()\n",
    "    cfg.input_path = './dataset/'\n",
    "    cfg.network_depth = 2\n",
    "    cfg.known_outliers = 30\n",
    "    cfg.cont_rate = 0.02\n",
    "    cfg.batch_size = 512\n",
    "    cfg.epochs = 60\n",
    "    cfg.lr = 2e-3\n",
    "    cfg.weight_decay = 2e-4\n",
    "    cfg.margin = 5.0\n",
    "    cfg.random_seed = 42\n",
    "\n",
    "    run_devnet(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d3062",
   "metadata": {},
   "source": [
    "# USING DIFFERENT PARAMETERS AND A FEW CHANGES IN THE MAIN CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4a43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 773us/step\n",
      "annthyroid_21feat_normalised: ROC AUC=0.9886, AUPR=0.9235\n",
      "194/194 [==============================] - 0s 646us/step\n",
      "bank-additional-full_normalised: ROC AUC=0.7992, AUPR=0.4185\n",
      "950/950 [==============================] - 1s 597us/step\n",
      "celeba_baldvsnonbald_normalised: ROC AUC=0.8532, AUPR=0.2112\n",
      "1403/1403 [==============================] - 1s 625us/step\n",
      "census-income-full-mixed-binarized: ROC AUC=0.7065, AUPR=0.2583\n",
      "1336/1336 [==============================] - 1s 595us/step\n",
      "creditcardfraud_normalised: ROC AUC=0.9703, AUPR=0.6953\n",
      "2904/2904 [==============================] - 3s 870us/step\n",
      "KDD2014_donors_10feat_nomissing_normalised: ROC AUC=1.0000, AUPR=0.9994\n",
      "447/447 [==============================] - 0s 957us/step\n",
      "UNSW_NB15_traintest_backdoor: ROC AUC=0.9690, AUPR=0.9086\n",
      "Summary results saved to all_dataset_results6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csc_matrix\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from joblib import Memory\n",
    "import pandas as pd\n",
    "\n",
    "mem = Memory(\"./dataset/svm_data\", verbose=0)\n",
    "\n",
    "def dataLoading(path):\n",
    "    df = pd.read_csv(path)\n",
    "    labels = df['class'].values\n",
    "    x = df.drop(['class'], axis=1).values\n",
    "    return x, labels\n",
    "\n",
    "@mem.cache\n",
    "def get_data_from_svmlight_file(path):\n",
    "    X, y = load_svmlight_file(path)\n",
    "    return X.toarray(), y\n",
    "\n",
    "class AUC_Callback(Callback):\n",
    "    def __init__(self, x_val, y_val):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "        if y_pred.shape[-1] == 1:\n",
    "            y_pred = y_pred.flatten()\n",
    "        val_aupr = average_precision_score(self.y_val, y_pred)\n",
    "        if logs is not None:\n",
    "            logs['val_aupr'] = val_aupr\n",
    "\n",
    "\n",
    "# loss func\n",
    "\n",
    "def create_deviation_loss(margin=5.0, ref_size=5000):\n",
    "    ref = K.variable(np.random.normal(size=ref_size), dtype='float32')\n",
    "    \n",
    "    def deviation_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, 'float32')  # Ensure y_true is float32\n",
    "        dev = (y_pred - K.mean(ref)) / (K.std(ref) + K.epsilon())\n",
    "        inlier_loss = K.abs(dev)\n",
    "        outlier_loss = K.abs(K.maximum(margin - dev, 0.0))\n",
    "        return K.mean((1 - y_true) * inlier_loss + y_true * outlier_loss)\n",
    "    \n",
    "    return deviation_loss\n",
    "\n",
    "\n",
    "#multiple architec for testing\n",
    "def dev_network_d(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(250, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "def dev_network_s(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "def dev_network_linear(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    out = Dense(1, activation='linear')(inp)\n",
    "    return Model(inp, out)\n",
    "\n",
    "def deviation_network(input_shape, depth, lr, wd, margin):\n",
    "    if depth == 4:\n",
    "        model = dev_network_d(input_shape)\n",
    "    elif depth == 2:\n",
    "        model = dev_network_s(input_shape)\n",
    "    elif depth == 1:\n",
    "        model = dev_network_linear(input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported network depth\")\n",
    "    optimizer = AdamW(learning_rate=lr, weight_decay=wd)\n",
    "    loss_fn = create_deviation_loss(margin)\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# oversample strat added\n",
    "def batch_generator_sup(x, out_idx, in_idx, batch_size, rng):\n",
    "    n_out_batch = max(1, batch_size // 2)\n",
    "    while True:\n",
    "        out_samples = rng.choice(out_idx, n_out_batch, replace=True)\n",
    "        in_samples = rng.choice(in_idx, batch_size - n_out_batch, replace=False)\n",
    "        batch_idx = np.concatenate([in_samples, out_samples])\n",
    "        rng.shuffle(batch_idx)\n",
    "        labels = np.isin(batch_idx, out_idx).astype(np.float32)\n",
    "        yield x[batch_idx], labels\n",
    "\n",
    "# vectorized noise injection\n",
    "def inject_noise(seed, n_out, random_seed):\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    n_sample, dim = seed.shape\n",
    "    swap_ratio = 0.05\n",
    "    n_swap = int(dim * swap_ratio)\n",
    "\n",
    "    i1 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    i2 = rng.choice(n_sample, size=n_out, replace=True)\n",
    "    idxs = rng.choice(dim, size=(n_out, n_swap), replace=True)\n",
    "\n",
    "    noise = seed[i1].copy()\n",
    "    rows = np.arange(n_out)[:, None]\n",
    "    noise[rows, idxs] = seed[i2[:, None], idxs]\n",
    "\n",
    "    return noise\n",
    "\n",
    "# network training and testing\n",
    "\n",
    "def run_devnet(config):\n",
    "    all_results = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for fname in os.listdir(config.input_path):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        name = fname.rsplit('.', 1)[0]\n",
    "        x, y = dataLoading(os.path.join(config.input_path, fname))\n",
    "\n",
    "        x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, stratify=y, random_state=config.random_seed)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=config.random_seed)\n",
    "\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        out_idx = np.where(y_train == 1)[0]\n",
    "        in_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "        if len(out_idx) > config.known_outliers:\n",
    "            drop = np.random.choice(out_idx, len(out_idx) - config.known_outliers, replace=False)\n",
    "            keep = np.setdiff1d(np.arange(len(y_train)), drop)\n",
    "            x_train, y_train = x_train[keep], y_train[keep]\n",
    "            out_idx = np.where(y_train == 1)[0]\n",
    "            in_idx = np.where(y_train == 0)[0]\n",
    "\n",
    "        if len(out_idx) > 0\n",
    "            n_synth_outliers = int(len(out_idx) * config.synth_factor)\n",
    "            synth_outliers = inject_noise(x_train[out_idx], n_synth_outliers, config.random_seed)\n",
    "            \n",
    "            x_train = np.vstack([x_train, synth_outliers])\n",
    "            y_train = np.concatenate([y_train, np.ones(n_synth_outliers)])\n",
    "\n",
    "        n_synth_inliers = int(len(in_idx) * config.cont_rate / (1 - config.cont_rate))\n",
    "        if n_synth_inliers > 0:\n",
    "            synth_inliers = inject_noise(x_train[in_idx], n_synth_inliers, config.random_seed)\n",
    "            x_train = np.vstack([x_train, synth_inliers])\n",
    "            y_train = np.concatenate([y_train, np.zeros(n_synth_inliers)])\n",
    "\n",
    "        out_idx = np.where(y_train == 1)[0]\n",
    "        in_idx = np.where(y_train == 0)[0]\n",
    "        model = deviation_network(\n",
    "            input_shape=(x_train.shape[1],),\n",
    "            depth=config.network_depth,\n",
    "            lr=config.lr,\n",
    "            wd=config.weight_decay,\n",
    "            margin=config.margin\n",
    "        )\n",
    "        ckpt = ModelCheckpoint(f\"./model/devnet_{name}.keras\", save_best_only=True, monitor='val_aupr', mode='max')\n",
    "        es_cb = EarlyStopping(monitor='val_aupr', mode='max', patience=10, restore_best_weights=True)\n",
    "        lr_cb = ReduceLROnPlateau(monitor='val_aupr', mode='max', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        auc_cb = AUC_Callback(x_val, y_val)\n",
    "\n",
    "        steps = max(1, len(in_idx) // config.batch_size)\n",
    "        model.fit(\n",
    "            batch_generator_sup(x_train, out_idx, in_idx, config.batch_size, np.random),\n",
    "            steps_per_epoch=steps,\n",
    "            epochs=config.epochs,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[auc_cb, ckpt, lr_cb, es_cb],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # eval\n",
    "        y_score = model.predict(x_test)\n",
    "        roc = roc_auc_score(y_test, y_score)\n",
    "        aupr = average_precision_score(y_test, y_score)\n",
    "        print(f\"{name}: ROC AUC={roc:.4f}, AUPR={aupr:.4f}\")\n",
    "        all_results.append({'dataset': name, 'roc': roc, 'aupr': aupr})\n",
    "\n",
    "    # results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    output_csv_filename = \"all_dataset_results6\"\n",
    "    results_df.to_csv(output_csv_filename, index=False)\n",
    "    print(\"Summary results saved to\", output_csv_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Config:\n",
    "        pass\n",
    "    \n",
    "    cfg = Config()\n",
    "    cfg.input_path = './dataset/'\n",
    "    cfg.network_depth = 2\n",
    "    cfg.known_outliers = 30\n",
    "    cfg.cont_rate = 0.02 \n",
    "    cfg.synth_factor = 0.5  # gen 50% synth outliers\n",
    "    cfg.batch_size = 512\n",
    "    cfg.epochs = 60\n",
    "    cfg.lr = 2e-3\n",
    "    cfg.weight_decay = 2e-4\n",
    "    cfg.margin = 5.0\n",
    "    cfg.random_seed = 42\n",
    "\n",
    "    run_devnet(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571ad78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
