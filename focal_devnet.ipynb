{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c331a0",
   "metadata": {},
   "source": [
    "## importds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7fca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf5a42",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f259b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./dataset/svm_data\", verbose=0)\n",
    "\n",
    "def dataLoading(path):\n",
    "    df = pd.read_csv(path)\n",
    "    y = df['class'].values\n",
    "    X = df.drop(['class'], axis=1).values\n",
    "    return X, y\n",
    "\n",
    "@mem.cache\n",
    "def get_data_from_svmlight_file(path):\n",
    "    X_sp, y = load_svmlight_file(path)\n",
    "    return X_sp.toarray(), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8086ab",
   "metadata": {},
   "source": [
    "## loss and call back built for maximizing aucpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loss tries to pay more attention on samples that are hard to learn\n",
    "# it uses focal idea on top of deviation so model focuses on big mistakes first\n",
    "# also it uses some random normal scores as reference and pushes real outliers away\n",
    "def create_focal_deviation_loss(margin=3.0, gamma=1.1, ref_size=5000):\n",
    "    ref = K.variable(np.random.normal(size=ref_size), dtype='float32')\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true_f = K.cast(y_true, 'float32')\n",
    "        dev = (y_pred - K.mean(ref)) / (K.std(ref) + K.epsilon())\n",
    "        in_l = K.abs(dev)\n",
    "        out_l = K.abs(K.maximum(margin - dev, 0.0))\n",
    "        base = (1.0 - y_true_f) * in_l + y_true_f * out_l\n",
    "        weight = K.pow(base / (K.max(base) + K.epsilon()), gamma)\n",
    "        return K.mean(weight * base)\n",
    "    return loss_fn\n",
    "\n",
    "# this callback is very simple it runs after each epoch\n",
    "# it sees how model does on val data by aupr\n",
    "# then stores it so training knows if it is good or not\n",
    "class AUC_Callback(Callback):\n",
    "    def __init__(self, x_val, y_val):\n",
    "        super().__init__()\n",
    "        self.x_val, self.y_val = x_val, y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x_val, verbose=0).flatten()\n",
    "        logs['val_aupr'] = average_precision_score(self.y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c96ec4",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this deep network has three big layers then one output\n",
    "# it uses batch norm, leaky relu and dropout to keep learning stable\n",
    "# dropout rate and activation slope come from config so we can tune easily\n",
    "def dev_network_d(input_shape, dropout_rate, activation_slope):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Dense(1000, kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = BatchNormalization()(x); x = LeakyReLU(alpha=activation_slope)(x); x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(250, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x); x = LeakyReLU(alpha=activation_slope)(x); x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(20, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x); x = LeakyReLU(alpha=activation_slope)(x); x = Dropout(dropout_rate)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# this function build the full deviation network using focal deviation loss\n",
    "# it takes config object so we keep code neat and parameters easy to change\n",
    "def deviation_network(input_shape, cfg):\n",
    "    model = dev_network_d(input_shape, cfg.dropout_rate, cfg.activation_slope)\n",
    "    loss = create_focal_deviation_loss(cfg.margin, cfg.gamma)\n",
    "    optimizer = AdamW(learning_rate=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b5f4e",
   "metadata": {},
   "source": [
    "## bacthing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generator makes batches with equal mix of normal and outlier samples\n",
    "# it picks half batch from outliers with replacement and half from inliers without replacement\n",
    "# then shuffles and yields the data and labels indicating which are outliers\n",
    "def batch_generator_sup(x, out_idx, in_idx, batch_size, rng):\n",
    "    half = max(1, batch_size // 2)\n",
    "    while True:\n",
    "        o = rng.choice(out_idx, half, replace=True)\n",
    "        i = rng.choice(in_idx, batch_size - half, replace=False)\n",
    "        idx = np.concatenate([i, o])\n",
    "        rng.shuffle(idx)\n",
    "        yield x[idx], np.isin(idx, out_idx).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d6d9c",
   "metadata": {},
   "source": [
    "## train+test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6782e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function goes through each csv file and train devnet then test it\n",
    "# it splits data into train, val, test, scales features, drops extra outliers, and adds synthetic noise\n",
    "# callbacks to track validation aupr, stop early, adjust learning rate, and save best model\n",
    "# after training it predicts on test set and saves roc and aupr for each dataset\n",
    "def run_devnet(cfg):\n",
    "    results = []\n",
    "    scaler = StandardScaler()\n",
    "    for fname in os.listdir(cfg.input_path):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        name = os.path.splitext(fname)[0]\n",
    "        X, y = dataLoading(os.path.join(cfg.input_path, fname))\n",
    "        \n",
    "        # train/val/test split\n",
    "        x_tr, x_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=cfg.random_seed)\n",
    "        x_val, x_te, y_val, y_te = train_test_split(x_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=cfg.random_seed)\n",
    "\n",
    "        # scaling features\n",
    "        x_tr = scaler.fit_transform(x_tr)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_te = scaler.transform(x_te)\n",
    "\n",
    "        # drop extra outliers if more than known limit\n",
    "        out_idx, in_idx = np.where(y_tr == 1)[0], np.where(y_tr == 0)[0]\n",
    "        if len(out_idx) > cfg.known_outliers:\n",
    "            drop = np.random.choice(out_idx, len(out_idx) - cfg.known_outliers, replace=False)\n",
    "            keep = np.setdiff1d(np.arange(len(y_tr)), drop)\n",
    "            x_tr, y_tr = x_tr[keep], y_tr[keep]\n",
    "            out_idx, in_idx = np.where(y_tr == 1)[0], np.where(y_tr == 0)[0]\n",
    "\n",
    "        # inject synthetic noise by duplicating outliers\n",
    "        n_noise = int(len(in_idx) * cfg.cont_rate / (1 - cfg.cont_rate))\n",
    "        synth = x_tr[np.random.choice(out_idx, n_noise, replace=True)]\n",
    "        x_tr = np.vstack([x_tr, synth])\n",
    "        y_tr = np.concatenate([y_tr, np.zeros(n_noise)])\n",
    "        in_idx = np.where(y_tr == 0)[0]\n",
    "\n",
    "        # build and compile model\n",
    "        model = deviation_network((x_tr.shape[1],), cfg)\n",
    "        callbacks = [\n",
    "            AUC_Callback(x_val, y_val),\n",
    "            EarlyStopping(monitor='val_aupr', mode='max', patience=2, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_aupr', mode='max', factor=0.5, patience=2, min_lr=1e-6),\n",
    "            ModelCheckpoint(os.path.join(cfg.model_dir, f\"devnet_{name}.keras\"), save_best_only=True, monitor='val_aupr', mode='max')\n",
    "        ]\n",
    "        steps = max(1, len(in_idx) // cfg.batch_size)\n",
    "        model.fit(\n",
    "            batch_generator_sup(x_tr, out_idx, in_idx, cfg.batch_size, np.random),\n",
    "            steps_per_epoch=steps,\n",
    "            epochs=20,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # evaluate on test set and collect metrics\n",
    "        y_score = model.predict(x_te).flatten()\n",
    "        results.append({\n",
    "            'dataset': name,\n",
    "            'roc': roc_auc_score(y_te, y_score),\n",
    "            'aupr': average_precision_score(y_te, y_score)\n",
    "        })\n",
    "\n",
    "    # save results to csv\n",
    "    pd.DataFrame(results).to_csv('focal_output_final.csv', index=False)\n",
    "    print(\"Focal loss results saved to focal_devnet_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72c3cc",
   "metadata": {},
   "source": [
    "## call to train+test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 4ms/step\n",
      "194/194 [==============================] - 1s 3ms/step\n",
      "950/950 [==============================] - 4s 4ms/step\n",
      "1403/1403 [==============================] - 6s 4ms/step\n",
      "1336/1336 [==============================] - 4s 3ms/step\n",
      "2904/2904 [==============================] - 10s 3ms/step\n",
      "447/447 [==============================] - 2s 4ms/step\n",
      "Focal loss results saved to focal_devnet_results.csv\n"
     ]
    }
   ],
   "source": [
    "# choose network size: 1=linear, 2=shallow, 4=deep deviation network\n",
    "# known_outliers: num outliers to bbe kept\n",
    "# contam = num outliers\n",
    "# lr =  learning rate\n",
    "# weight decay = L2 regularization\n",
    "# margin = how far outliers must deviate from normal\n",
    "# gamma = focus opn harder examples (high gamma = more focus to data clsoer to margin)\n",
    "# act slope ==  leaky relu constraint\n",
    "if __name__ == \"__main__\":\n",
    "    class Config: pass\n",
    "\n",
    "    cfg = Config()\n",
    "    cfg.input_path = './dataset/'\n",
    "    cfg.model_dir = './model'\n",
    "    cfg.network_depth = 4\n",
    "    cfg.known_outliers = 30\n",
    "    cfg.cont_rate = 0.02\n",
    "    cfg.batch_size = 512\n",
    "    cfg.lr = 1e-2\n",
    "    cfg.weight_decay = 1e-5\n",
    "    cfg.random_seed = 42\n",
    "    cfg.dropout_rate = 0.3\n",
    "    cfg.activation_slope = 0.1\n",
    "    cfg.margin = 3.0\n",
    "    cfg.gamma = 1.1\n",
    "\n",
    "    os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "    run_devnet(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571ad78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
